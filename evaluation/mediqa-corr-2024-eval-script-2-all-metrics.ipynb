{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Evaluation Script for MEDIQA-CORR 2024**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "from rouge import Rouge\n",
        "import bert_score.score as bertscore\n",
        "import bleurt.score as bleurtscore\n",
        "import scipy.stats as stats\n",
        "import numpy as np\n",
        "import math\n",
        "import string"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1707888457570
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#################\n",
        "# Parsing Funcs #\n",
        "#################\n",
        "\n",
        "\n",
        "def parse_reference_file(filepath):\n",
        "    \"\"\"Parsing reference file path.\n",
        "\n",
        "    Returns:\n",
        "        reference_corrections (dict) {text_id: \"reference correction\"}\n",
        "        reference_flags (dict) {text_id: \"1 or 0 error flag\"}\n",
        "        reference_sent_id (dict) {text_id: \"error sentence id or -1\"}\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    reference_corrections = {}\n",
        "    reference_flags = {}\n",
        "    reference_sent_id = {}\n",
        "\n",
        "    df = pd.read_csv(filepath)\n",
        "    \n",
        "    for index, row in df.iterrows():\n",
        "        text_id = row['Text ID']\n",
        "        corrected_sentence = row['Corrected Sentence']\n",
        "        \n",
        "        if not isinstance(corrected_sentence, str):\n",
        "            if math.isnan(corrected_sentence):\n",
        "                corrected_sentence = \"NA\"\n",
        "            else:\n",
        "                corrected_sentence = str(corrected_sentence)\n",
        "                corrected_sentence = corrected_sentence.replace(\"\\n\", \" \") \\\n",
        "                  .replace(\"\\r\", \" \").strip()\n",
        "                  \n",
        "        reference_corrections[text_id] = corrected_sentence\n",
        "        reference_flags[text_id] = str(row['Error Flag'])\n",
        "        reference_sent_id[text_id] = str(row['Error Sentence ID'])\n",
        "\n",
        "    return reference_corrections, reference_flags, reference_sent_id\n",
        "\n",
        "\n",
        "def parse_run_submission_file(filepath):\n",
        "    \n",
        "    file = open(filepath, 'r')\n",
        "\n",
        "    candidate_corrections = {}\n",
        "    predicted_flags = {}\n",
        "    candidate_sent_id = {}\n",
        "    \n",
        "    lines = file.readlines()\n",
        "    \n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        \n",
        "        if len(line) == 0:\n",
        "            continue\n",
        "            \n",
        "        if not re.fullmatch('[a-z0-9\\-]+\\s[0-9]+\\s\\-?[0-9]+\\s.+', line):\n",
        "            print(\"Invalid line: \", line)\n",
        "            continue\n",
        "            \n",
        "        # replacing consecutive spaces\n",
        "        line = re.sub('\\s+', line, ' ')\n",
        "        \n",
        "        # parsing\n",
        "        items = line.split()\n",
        "        text_id = items[0]\n",
        "        error_flag = items[1]\n",
        "        sentence_id = items[2]\n",
        "        corrected_sentence = ' '.join(items[3:]).strip()\n",
        "        \n",
        "        # debug - parsing check\n",
        "        # print(\"{} -- {} -- {} -- {}\".format(text_id, error_flag, sentence_id, corrected_sentence))\n",
        "\n",
        "        predicted_flags[text_id] = error_flag\n",
        "        candidate_sent_id[text_id] = sentence_id\n",
        "\n",
        "        # processing candidate corrections\n",
        "        # removing quotes\n",
        "\n",
        "        while corrected_sentence.startswith('\"') and len(corrected_sentence) > 1:\n",
        "            corrected_sentence = corrected_sentence[1:]\n",
        "            \n",
        "        while corrected_sentence.endswith('\"') and len(corrected_sentence) > 1:\n",
        "            corrected_sentence = corrected_sentence[:-1]\n",
        "                   \n",
        "        if error_flag == '0':\n",
        "            # enforcing \"NA\" in predicted non-errors (used for consistent/reliable eval)\n",
        "            candidate_corrections[text_id] = \"NA\"\n",
        "        else:\n",
        "            candidate_corrections[text_id] = corrected_sentence\n",
        "\n",
        "    return candidate_corrections, predicted_flags, candidate_sent_id\n",
        "            "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1707888457719
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##############\n",
        "# Eval Funcs #\n",
        "##############\n",
        "\n",
        "def compute_accuracy(reference_flags, reference_sent_id, predicted_flags, candidate_sent_id):\n",
        "    # Error Flags Accuracy (missing predictions are counted as false)\n",
        "    matching_flags_nb = 0\n",
        "    \n",
        "    for text_id in reference_flags:\n",
        "        if text_id in predicted_flags and reference_flags[text_id] == predicted_flags[text_id]:\n",
        "            matching_flags_nb += 1\n",
        "            \n",
        "    flags_accuracy = matching_flags_nb / len(reference_flags)\n",
        "    \n",
        "    # Error Sentence Detection Accuracy (missing predictions are counted as false)\n",
        "    matching_sentence_nb = 0\n",
        "    \n",
        "    for text_id in reference_sent_id:\n",
        "        if text_id in candidate_sent_id and candidate_sent_id[text_id] == reference_sent_id[text_id]:\n",
        "            matching_sentence_nb += 1\n",
        "            \n",
        "    sent_accuracy = matching_sentence_nb / len(reference_sent_id)\n",
        "\n",
        "    return {\n",
        "        \"Error Flags Accuracy\": flags_accuracy,\n",
        "        \"Error Sentence Detection Accuracy\": sent_accuracy\n",
        "    }\n",
        "\n",
        "def increment_counter(counters, counter_name):\n",
        "    counters[counter_name] = counters[counter_name] + 1\n",
        "\n",
        "def clip(value): # clip to a 0-1 value\n",
        "    return max(0, min(1, value))\n",
        "\n",
        "class NLGMetrics(object):\n",
        "\n",
        "    def __init__(self, metrics = ['ROUGE', 'BERTSCORE', 'BLEURT']): ## default metrics\n",
        "        self.metrics = metrics\n",
        "    \n",
        "    def compute(self, references, predictions, counters):\n",
        "        results = {}\n",
        "\n",
        "        assert len(predictions) == len(references), \"Predictions and references do not have the same size.\"\n",
        "        \n",
        "        results['aggregate_subset_check'] = np.array([0 for x in range(len(predictions))])\n",
        "        aggregate_components = 0\n",
        "\n",
        "        if 'ROUGE' in self.metrics:\n",
        "            rouge = Rouge() \n",
        "            rouge_scores = rouge.get_scores(predictions, references)\n",
        "                            \n",
        "            rouge1f_scores = []\n",
        "            rouge2f_scores = []\n",
        "            rougeLf_scores = []\n",
        "            \n",
        "            for i in range(len(references)):\n",
        "                r1f = rouge_scores[i][\"rouge-1\"][\"f\"]\n",
        "                r2f = rouge_scores[i][\"rouge-2\"][\"f\"]\n",
        "                rlf = rouge_scores[i][\"rouge-l\"][\"f\"]\n",
        "                \n",
        "                rouge1f_scores.append(r1f)\n",
        "                rouge2f_scores.append(r2f)\n",
        "                rougeLf_scores.append(rlf)\n",
        "                \n",
        "            # for checking comparison with composite\n",
        "            rouge1check = np.array(rouge1f_scores).mean()\n",
        "            rouge2check = np.array(rouge2f_scores).mean()\n",
        "            rougeLcheck = np.array(rougeLf_scores).mean()\n",
        "\n",
        "            results['R1F_subset_check'] = rouge1check\n",
        "            results['R2F_subset_check'] = rouge2check\n",
        "            results['RLF_subset_check'] = rougeLcheck\n",
        "\n",
        "            # rouge-1-f is used for the aggregate score\n",
        "            # sum element-wise for later aggregate score computation\n",
        "            results['aggregate_subset_check'] = results['aggregate_subset_check'] + np.array(rouge1f_scores)\n",
        "            aggregate_components += 1\n",
        "            \n",
        "            ###############################\n",
        "            # Composite score computation #\n",
        "            ###############################\n",
        "            \n",
        "            \"\"\"\n",
        "            NLG METRIC on sentence vs. sentence cases + ones or zeros \n",
        "            when either the reference or the candidate correction is NA\n",
        "            \"\"\"\n",
        "            \n",
        "            rouge1score = np.array(rouge1f_scores).sum()\n",
        "            rouge2score = np.array(rouge2f_scores).sum()\n",
        "            rougeLscore = np.array(rougeLf_scores).sum()\n",
        "            \n",
        "            composite_score_rouge1 = (rouge1score + counters[\"system_provided_correct_na\"]) / counters[\"total_texts\"]\n",
        "            composite_score_rouge2 = (rouge2score + counters[\"system_provided_correct_na\"]) / counters[\"total_texts\"]\n",
        "            composite_score_rougeL = (rougeLscore + counters[\"system_provided_correct_na\"]) / counters[\"total_texts\"]\n",
        "\n",
        "            results['R1FC'] = composite_score_rouge1\n",
        "            results['R2FC'] = composite_score_rouge2\n",
        "            results['RLFC'] = composite_score_rougeL\n",
        "\n",
        "        if 'BERTSCORE' in self.metrics:\n",
        "            bertScore_Precision, bertScore_Recall, bertScore_F1 = bertscore(predictions, references, model_type='microsoft/deberta-xlarge-mnli', lang='en', device ='cpu' , verbose=True, rescale_with_baseline=True) # roberta-large\n",
        "            \n",
        "            bertscores = bertScore_F1.numpy()\n",
        "            ## clip scores to [0,1]\n",
        "            bertscores = np.array([clip(num) for num in bertscores])\n",
        "\n",
        "            results['BERTSCORE_subset_check'] = bertscores.mean()\n",
        "            composite_score_bert = (bertscores.sum() + counters[\"system_provided_correct_na\"]) / counters[\"total_texts\"]\n",
        "            results['BERTC'] = composite_score_bert\n",
        "\n",
        "            # sum element-wise for later aggregate score computation\n",
        "            results['aggregate_subset_check'] = results['aggregate_subset_check'] + np.array(bertscores)\n",
        "            aggregate_components += 1\n",
        "\n",
        "            \n",
        "        if 'BLEURT' in self.metrics:\n",
        "            bleurtscorer = bleurtscore.BleurtScorer(checkpoint=\"BLEURT-20\")\n",
        "            \n",
        "            bleurtscores = bleurtscorer.score(references=references, candidates=predictions, batch_size =1)\n",
        "            ## clip scores to [0,1]\n",
        "            bleurtscores = np.array([clip(num) for num in bleurtscores])\n",
        "\n",
        "            results['BLEURT_subset_check'] = bleurtscores.mean()\n",
        "            composite_score_bleurt = (bleurtscores.sum() + counters[\"system_provided_correct_na\"]) / counters[\"total_texts\"]\n",
        "            results['BLEURTC'] = composite_score_bleurt\n",
        "\n",
        "            # sum element-wise for later aggregate score computation\n",
        "            results['aggregate_subset_check'] = results['aggregate_subset_check'] + np.array(bleurtscores) \n",
        "                    \n",
        "            aggregate_components += 1\n",
        "\n",
        "        if aggregate_components > 0:\n",
        "            aggregate_subset_scores = results['aggregate_subset_check'] / aggregate_components\n",
        "            composite_score_agg = (aggregate_subset_scores.sum() + counters[\"system_provided_correct_na\"]) / counters[\"total_texts\"]\n",
        "            \n",
        "            results['aggregate_subset_check'] = aggregate_subset_scores.mean() \n",
        "            results['AggregateC'] = composite_score_agg\n",
        "\n",
        "            \n",
        "\n",
        "        return results\n",
        "\n",
        "\n",
        "def get_nlg_eval_data(reference_corrections, candidate_corrections, remove_nonprint = False):\n",
        "    references = []\n",
        "    predictions = []\n",
        "    \n",
        "    counters = {\n",
        "        \"total_texts\": 0,\n",
        "        \"reference_na\": 0,\n",
        "        \"total_system_texts\": 0,\n",
        "        \"system_provided_na\": 0,\n",
        "        \"system_provided_correct_na\": 0,\n",
        "    }\n",
        "    \n",
        "    for text_id in reference_corrections:\n",
        "        increment_counter(counters, \"total_texts\")\n",
        "        \n",
        "        # removing non ascii chars\n",
        "        reference_correction = reference_corrections[text_id]\n",
        "        \n",
        "        if remove_nonprint:\n",
        "            reference_correction = ''.join(filter(lambda x: x in string.printable, str(reference_correction)))\n",
        "            \n",
        "        if reference_correction == \"NA\":\n",
        "            increment_counter(counters, \"reference_na\")\n",
        "            \n",
        "        if text_id in candidate_corrections:\n",
        "            increment_counter(counters, \"total_system_texts\")\n",
        "            candidate = candidate_corrections[text_id]\n",
        "            \n",
        "            if remove_nonprint:\n",
        "                candidate = ''.join(filter(lambda x: x in string.printable, candidate))\n",
        "                \n",
        "            if candidate == \"NA\":\n",
        "                increment_counter(counters, \"system_provided_na\")\n",
        "                \n",
        "            # matching NA counts as 1\n",
        "            if reference_correction == \"NA\" and candidate == \"NA\":\n",
        "                increment_counter(counters, \"system_provided_correct_na\")\n",
        "                continue\n",
        "                \n",
        "            # Run provided \"NA\" when a correction was required (=> 0)\n",
        "            # or Run provided a correction when \"NA\" was required (=> 0)\n",
        "            if candidate == \"NA\" or reference_correction == \"NA\":\n",
        "                continue\n",
        "                \n",
        "            # remaining case is both reference and candidate are not \"NA\"\n",
        "            # both are inserted/added for ROUGE/BLEURT/etc. computation\n",
        "            references.append(reference_correction)\n",
        "            predictions.append(candidate)\n",
        "    \n",
        "    return references, predictions, counters\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1707888458074
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "submission_file = \"data/baseline-run-1.txt\"\n",
        "reference_csv_file = \"data/MEDIQA-CORR-2024-MS-ValidationSet-1.csv\"\n",
        "\n",
        "reference_corrections, reference_flags, reference_sent_id = parse_reference_file(reference_csv_file)\n",
        "candidate_corrections, candidate_flags, candidate_sent_id = parse_run_submission_file(submission_file)\n",
        "\n",
        "# Accuracy\n",
        "accuracy_results = compute_accuracy(reference_flags, reference_sent_id, candidate_flags, candidate_sent_id)\n",
        "print(\"Accuracy Results:\\n\", accuracy_results)\n",
        "print()\n",
        "\n",
        "# NLG Eval for corrections\n",
        "references, predictions, counters = get_nlg_eval_data(reference_corrections, candidate_corrections)\n",
        "metrics = NLGMetrics(metrics = ['ROUGE', 'BERTSCORE', 'BLEURT']) # metrics = ['ROUGE', 'BERTSCORE', 'BLEURT']\n",
        "nlg_eval_results = metrics.compute(references, predictions, counters) \n",
        "\n",
        "\n",
        "print(\"NLG Eval Results:\\n\", nlg_eval_results) \n",
        "print()\n",
        "\n",
        "# debug check\n",
        "print(counters)\n",
        "print()\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1707888084660
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "kernel_info": {
      "name": "python38-azureml"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}